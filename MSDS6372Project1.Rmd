---
title: "MSDS6372Project1"
author: "Nicole Norelli, Alex Gilbert & Mingyang Nick YU"
date: "1/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Automible MSRP Analysis and Prediction

**Data Import and Cleaning...**
```{r dataINC}
library(tidyverse)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project1")
# Read in data under variable CarData
CarData = read.csv("data1.csv")
# Fixing three empty values in CarData$Engine.Fuel.Type, after research they are regular unleaded
CarData$Engine.Fuel.Type = ifelse(CarData$Engine.Fuel.Type=="","regular unleaded",CarData$Engine.Fuel.Type)
# Fixing Engine Cylinders to 0 -- Mazda RX or Electrical Vehicle
CarData$Engine.Cylinders = ifelse(is.na(CarData$Engine.Cylinders),0,CarData$Engine.Cylinders)
# Fixing Tesla Model S with Number.of.Doors missing
CarData$Number.of.Doors = ifelse(CarData$Make=="Tesla"&is.na(CarData$Number.of.Doors),4,CarData$Number.of.Doors)
# Fixing Tesla HP with mean HP found online of 510
#CarData$Number.of.Doors = ifelse(CarData$Make=="Tesla"&is.na(CarData$Number.of.Doors),4,CarData$Number.of.Doors)
# Fixing Ferrari FF 2013 Number.of.Doors to 2 -- Google research
CarData$Number.of.Doors = ifelse(CarData$Make=="Ferrari"&is.na(CarData$Number.of.Doors),2,CarData$Number.of.Doors)
# Fixing FIAT 500e HP NA value based on research on mean Horsepower
CarData$Engine.HP = ifelse(CarData$Make=="FIAT"&is.na(CarData$Engine.HP),111,CarData$Engine.HP)
# Fixing Lincoln Continental HP NA value based on research on mean Horsepower
CarData$Engine.HP = ifelse(CarData$Make=="Lincoln"&is.na(CarData$Engine.HP),345,CarData$Engine.HP)
# Fixing Ford Escape HP NA value based on research on mean Horsepower
CarData$Engine.HP = ifelse(CarData$Make=="Ford"&is.na(CarData$Engine.HP),210,CarData$Engine.HP)
# Fixing Honda Fit EV HP NA value based on research on mean Horsepower
CarData$Engine.HP = ifelse(CarData$Make=="Honda"&is.na(CarData$Engine.HP),123,CarData$Engine.HP)
# Fixing Mitsubishi i-MiEV HP NA value based on research on mean Horsepower
CarData$Engine.HP = ifelse(CarData$Make=="Mitsubishi"&is.na(CarData$Engine.HP),66,CarData$Engine.HP)
# Fixing Chevrolet  Impala HP NA value based on research on mean Horsepower
CarData$Engine.HP = ifelse(CarData$Make=="Chevrolet"&is.na(CarData$Engine.HP),305,CarData$Engine.HP)
# Fixing Nissan Leaf HP NA value based on research on mean Horsepower
CarData$Engine.HP = ifelse(CarData$Make=="Nissan"&is.na(CarData$Engine.HP),177,CarData$Engine.HP)
# Fixing Tesla Model S HP NA value based on research on mean Horsepower
CarData$Engine.HP = ifelse(CarData$Make=="Tesla"&is.na(CarData$Engine.HP),503,CarData$Engine.HP)
# Fixing Mercedes-Benz M-Class S HP NA value based on research on mean Horsepower
CarData$Engine.HP = ifelse(CarData$Make=="Mercedes-Benz"&is.na(CarData$Engine.HP),350,CarData$Engine.HP) 
# Fixing Toyota RAV4 HP NA value based on research on mean Horsepower
CarData$Engine.HP = ifelse(CarData$Make=="Toyota"&is.na(CarData$Engine.HP),203,CarData$Engine.HP) 
# Fixing Kia Soul EV HP NA value based on research on mean Horsepower
CarData$Engine.HP = ifelse(CarData$Make=="Kia"&is.na(CarData$Engine.HP),109,CarData$Engine.HP) 

#Market.Category: Crossover,Diesel,Exotic,Luxury,High-Performance,Factory Tuner,Performance,Flex Fuel,Hatchback,Hybrid,N/A (Note: Performance column will be handled individually becuase of presence of High-Performance)
marketCat = c("Crossover","Diesel","Exotic","Luxury","High-Performance","Factory Tuner","Flex Fuel","Hatchback","Hybrid","N/A")
marketCat = data.frame(marketCat)

# Add each Market Category as column
CarData['Crossover'] <- NA
CarData['Diesel'] <- NA
CarData['Exotic'] <- NA
CarData['Luxury'] <- NA
CarData['High-Performance'] <- NA
CarData['Factory Tuner'] <- NA
CarData['Performance'] <- NA
CarData['Flex Fuel'] <- NA
CarData['Hatchback'] <- NA
CarData['Hybrid'] <- NA
CarData['N/A'] <- NA

str(CarData)
# Use For loop to fill in Yes/No to each Market Category column
for(j in 1:dim(marketCat)[1]){
  CarData[marketCat[j,1]] = ifelse(grepl(marketCat[j,1],CarData$Market.Category),"Yes","No")
}
# Performance is being handled individually...
CarData$Performance = ifelse(grepl("Performance",CarData$Market.Category)&(!grepl("High-Performance",CarData$Market.Category)),"Yes","No")

# Convert factor variables
variables.to.factor = c("Make","Model","Engine.Fuel.Type","Transmission.Type","Driven_Wheels","Market.Category","Vehicle.Size","Vehicle.Style","Crossover","Diesel","Exotic","Luxury","High-Performance","Factory Tuner","Performance","Flex Fuel","Hatchback","Hybrid","N/A")
CarData[variables.to.factor] = lapply(CarData[variables.to.factor],factor)

# Adjust column names - get ride of - and space in the column names to make analysis easier
# Also make column names more uniform
CarData = CarData %>%
  rename(
    EngineFuelType = Engine.Fuel.Type,
    EngineHP = Engine.HP,
    EngineCylinders = Engine.Cylinders,
    TransmissionType = Transmission.Type,
    DrivenWheels = Driven_Wheels,
    NumberOfDoors = Number.of.Doors,
    MarketCategory = Market.Category,
    VehicleSize = Vehicle.Size,
    VehicleStyle = Vehicle.Style,
    HighwayMPG = highway.MPG,
    CityMPG = city.mpg,
    HighPerformance = `High-Performance`,
    FactoryTuner = `Factory Tuner`,
    FlexFuel = `Flex Fuel`,
    N_A = `N/A`
  )
# Look at HighwayMPG outlier identified by Nicole
CarData %>% filter(HighwayMPG==354)
# Change observation HighwayMPG 354 to 34
CarData$HighwayMPG = ifelse(CarData$HighwayMPG==354,34,CarData$HighwayMPG)

# Look at summary of data
summary(CarData$VehicleStyle)
str(CarData)
# Look at missing Data if any
# missingData = CarData[rowSums(is.na(CarData)) > 0,]
# missingData
# dim(missingData)
write.csv(CarData,"CleanedCarData.csv",row.names = FALSE)

```

# Start Here... Load in cleanedData, delete over 1 million dollar cars
## split dataset into train/test/validate sets with set seed number
### Models that only have 5 observations will all end up in train set
```{r}
library(tidyverse)
library(ggplot2)
library(GGally)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project1")
# Directly import cleaned dataset and start from here
CarData = read.csv("CleanedCarData.csv")
###################################################################################
# Restrict prediction range to over cars under 1 million... To improve precision###
###################################################################################
CarData = CarData %>% filter(MSRP < 1000000)
# Convert factor variables
variables.to.factor = c("Make","Model","EngineFuelType","TransmissionType","DrivenWheels","MarketCategory","VehicleSize","VehicleStyle","Crossover","Diesel","Exotic","Luxury","HighPerformance","FactoryTuner","Performance","FlexFuel","Hatchback","Hybrid","N_A")
CarData[variables.to.factor] = lapply(CarData[variables.to.factor],factor)
summary(CarData)
# Plot agsint numeric variables
# CarData %>% select(MSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity) %>% ggpairs()
CarData = CarData[,-10] #delete MarketCategory
# Try different way of sampling by Models

######### Try Something else to split data ############
# First 836 are Models that only have 5 observations or less - deleted 6 card over 1 million dollars
ModelList = CarData%>% group_by(Model) %>% mutate(ModelCount = n()) %>% arrange(ModelCount)
#arrange(ModelList,desc(count))%>%print(n = Inf)
# Can only split train/test/validate set on Models that has 6 observations or more
ModelList = ModelList[,-27]
Not.Able.To.Split = ModelList[(1:836),]
Able.To.Split = ModelList[-(1:836),]

# Split Train, test, validate based on able to Train Data...
set.seed(111)
validate.set.index<-sample(1:dim(Able.To.Split)[1],1191,replace=F)
validate = Able.To.Split[validate.set.index,]
remains1 = Able.To.Split[-validate.set.index,]
set.seed(112)
test.index = sample(1:dim(remains1)[1],1191,replace=F)
test = remains1[test.index,]
train = remains1[-test.index,]

# Joining train set with dataset that been left out
train = bind_rows(train,Not.Able.To.Split)

dim(train)
train = as.data.frame(train)
test = as.data.frame(test)
validate = as.data.frame(validate)
str(train)
str(CarData)
```

#### Objecive 1
We are trying to analyse the importance of the "Popularity" variable. While the details of this variable is vague, it was created from social media, and the "higher ups" are curious how much general popularity can play a role in the retail price of a vehicle.

This model need to be highly interpretable. We need to provide interpretation of the regression coefficients of the final model including hypothesis testing, interpretation of regression coefficients, and confidence intervals.

```{r}
# Explore numerical variables
train %>% select(MSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity)%>%ggpairs()


```

#### Objective 2 start without Log transformation - since there are so many categorical variables

```{r}
# Use Forward variable selection method to select possible variables as predictors
########################################
# Try Forward selection model.##########
########################################
library(leaps)
reg.fwd=regsubsets(MSRP~.,data=train,method="forward",nvmax=963)
par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:964,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.fwd)$adjr2
plot(1:964,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.fwd)$rss
plot(1:964,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)

# how many predictors gives the best BIC?
which(bics==min(bics))
coef(reg.fwd,349)
# By looking at this model, it has selected following predictors:
# Make+Model+Year+EngineFuelType+NumberOfDoors+VehicleSize+VehicleStyle+CityMPG+Luxury+TransmissionType+Exotic+Hatchback
forward.regression = lm(MSRP~Make+Model+Year+EngineFuelType+NumberOfDoors+VehicleSize+VehicleStyle+CityMPG+Luxury+TransmissionType+Exotic+Hatchback,data=train)
summary(forward.regression)
# This model has higher adjusted R-squared compared to the Log transformed model
par(mfrow=c(2,2))
plot(forward.regression)
library(olsrr)
ols_plot_cooksd_bar(forward.regression)
ols_plot_resid_stand(forward.regression)
# Cook's D plot looks okay 
# Residual plot... ???


########################################
# Try Backward elimination model.#######
########################################
reg.bwd=regsubsets(MSRP~.,data=train,method="backward",nvmax=963)
par(mfrow=c(1,3))
bics<-summary(reg.bwd)$bic
plot(1:964,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.bwd)$adjr2
plot(1:964,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.bwd)$rss
plot(1:964,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)

# how many predictors gives the best BIC?
which(bics==min(bics))
coef(reg.bwd,410)
# Predictor for backward elimination method based on lowest BIC value
# Make+Model+EngineFuelType+NumberOfDoors+VehicleSize+VehicleStyle+CityMPG+Luxury+FactoryTuner+FlexFuel+Exotic+Hatchback
backward.regression = lm(MSRP~Make+Model+EngineFuelType+NumberOfDoors+VehicleSize+VehicleStyle+CityMPG+Luxury+FactoryTuner+FlexFuel+Exotic+Hatchback,data=train)
summary(backward.regression)
# Forward select has higher adjusted R-squared compared to backward method
par(mfrow=c(2,2))
plot(backward.regression)

ols_plot_cooksd_bar(backward.regression)
ols_plot_resid_stand(backward.regression)
#Observation 3748, 9152 and 9153 seem to have problem on the residual plot - Same as forward selection


########################################
# Try LASSO variable selection #########
########################################
library(glmnet)
#Dummy code categorical predictor variables
x <- model.matrix(MSRP~.,train)[,-1]
y <- train$MSRP
set.seed(123) # Need a seed number to keep same results for LASSO
cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
par(mfrow=c(1,1))
plot(cv.out)
# Look at the coefficient for best lambda
# Make+Model+Year+EngineFuelType+EngineHP+EngineCylinders+TransmissionType+DrivenWheels+NumberOfDoors+VehicleSize+VehicleStyle+HighwayMPG+CityMPG+Popularity+Crossover+Diesel+Exotic+Luxury+HighPerformance+FactoryTurner+Performance+FlexFuel+Hatchback+Hybrid+Yes+N_A
# Lasso turned off some predictors in Make, Model,VehicleStyle... In order to check assumptions, we need to use lm model with all the predictors
# Although this won't imitate the Lasso model perfectly though.
coefficients = coef(cv.out, cv.out$lambda.min)
head(coefficients)
# Compute the final lasso model
lasso.model = glmnet(x,y,alpha = 1,lambda = cv.out$lambda.min)
# Check assumptions

#Dummy code test set to see performance compare to other methods
xtest<-model.matrix(MSRP~.,test)[,-1]
ytest<-test$MSRP #36302210

# See prediction
lasso.pred=predict(lasso.model ,newx=xtest)
# See test ASE
testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO

#####################################################
# Compare Forward vs. Backward on Test set to LASSO #
#####################################################
# Test result
test.result = test$MSRP
# str(test)
test = test[,-15] #delete MSRP for prediction
#Forward test results
forward.pred = predict(forward.regression,test)
# See forward test ASE
testMSE_Forward<-mean((test.result-forward.pred)^2)
testMSE_Forward #46054176

#Backward test results
backward.pred = predict(backward.regression,test)
# See backward test ASE
testMSE_Backward<-mean((test.result-backward.pred)^2)
testMSE_Backward #44935730

```










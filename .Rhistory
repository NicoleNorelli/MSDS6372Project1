model2 = lm(SalePrice~GrLivArea,data=data2)
ols_plot_resid_stand(model2)
ols_plot_cooksd_bar(model3)
ols_plot_cooksd_bar(model2)
plot(model3)
plot(model2)
xlab("Square Foot of Living Area")+
ylab("Sale Price")
data2 %>% ggplot(aes(x=GrLivArea,y=SalePrice))+
geom_point()+ggtitle("Sale Price vs. Square Foot of Living area")+
xlab("Square Foot of Living Area")+
ylab("Sale Price")
#Cook's Distance within roughly 0.1, residual within 3, decent amount of observations beyond 2 (within 5%)
#Assumptions met, move on to include categorical variables
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
plot(model3)
data2 %>% ggplot(aes(x=GrLivArea,y=SalePrice,color=Neighborhood))+
geom_point()+ggtitle("Sale Price vs. Square Foot of Living area")+
xlab("Square Foot of Living Area")+
ylab("Sale Price")
plot(model3)
ols_plot_resid_stand(model3)
ols_plot_cooksd_bar(model3)
m
summary(model3)
summary(model2)
anova(model3)
confint(model3)
summary(model3)
data2$Neighborhood = relevel(data2$Neighborhood,ref="Edwards")
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
confint(model3)
# Change reference to BrkSide to get CI for slope
data2$Neighborhood = relevel(data2$Neighborhood,ref="BrkSide")
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
confint(model3)
# Change reference to NAmes to get CI for slope
data2$Neighborhood = relevel(data2$Neighborhood,ref="NAmes")
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
confint(model3)
# convert certain columns we need into factor in order to use multi regression model
cols.to.factor = c("MSSubClass","MSZoning","Street","Alley","LotShape","LandContour","Utilities",
"LotConfig","LandSlope","Neighborhood","Condition1","Condition2","BldgType",
"HouseStyle","OverallQual","OverallCond","RoofStyle","RoofMatl","Exterior1st",
"Exterior2nd","MasVnrType","ExterQual","ExterCond","Foundation","BsmtQual",
"BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2","Heating","HeatingQC",
"CentralAir","Electrical","KitchenQual","Functional","FireplaceQu","GarageType",
"GarageFinish","GarageQual","GarageCond","PavedDrive","PoolQC","Fence","MiscFeature",
"SaleType","SaleCondition","PoolArea","MoSold","YrSold","Fireplaces","FullBath")
amesHouse[cols.to.factor] = lapply(amesHouse[cols.to.factor],factor)
#Now exploring relationships between continuous variables
amesHouse %>% dplyr::select(SalePrice,LotFrontage,LotArea,YearBuilt,YearRemodAdd,MasVnrArea,BsmtFinSF1,BsmtFinSF2)%>%
ggpairs()
#Try Logrithmic on SalePrice
amesHouse$lSalePrice = log(amesHouse$SalePrice)
#explore relationship between log SalePrice
amesHouse %>% dplyr::select(lSalePrice,LotFrontage,LotArea,YearBuilt,YearRemodAdd,MasVnrArea,BsmtFinSF1,BsmtFinSF2)%>%
ggpairs()
#Move forward with next batch of exploration
amesHouse %>% dplyr::select(SalePrice,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea) %>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea) %>%
ggpairs()
#Move forward with next batch of exploration
amesHouse %>% dplyr::select(SalePrice,BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr,
TotRmsAbvGrd)%>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea) %>%
ggpairs()
#Move forward with next batch of exploration
amesHouse %>% dplyr::select(SalePrice,BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr,
TotRmsAbvGrd)%>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr,
TotRmsAbvGrd)%>%
ggpairs()
#The only variable seem to have good correlation with lSalePrice is FullBath,TotRmsAbvGrd.
#???If we should change BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr to factors???
#Keep going
amesHouse %>% dplyr::select(SalePrice,GarageYrBlt,GarageCars,GarageArea) %>% ggpairs()
amesHouse %>% dplyr::select(lSalePrice,GarageYrBlt,GarageCars,GarageArea) %>% ggpairs()
#The above variables all seem to have a good linear correlation with lSalePrice
#Keep going
amesHouse %>% dplyr::select(SalePrice,WoodDeckSF,OpenPorchSF,EnclosedPorch,X3SsnPorch,ScreenPorch,MiscVal)%>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,WoodDeckSF,OpenPorchSF,EnclosedPorch,X3SsnPorch,ScreenPorch,MiscVal)%>%
ggpairs()
#Loading libraries needed
library(tidyverse)
library(ggplot2)
#Loading in Employee data
employeeData = read.csv("/Users/mingyang/Desktop/SMU/DoingDS_Fall2020/CaseStudy2DDS/CaseStudy2-data.csv",header = TRUE)
summary(employeeData)
#See which column has only unique value
sapply(employeeData,function(col) length(unique(col)))
#Delete column that has only one unique value
to.be.deleted = which(sapply(employeeData,function(col) length(unique(col))==1))
employeeData = employeeData[,-to.be.deleted]
#Convert some values into factors
cols.to.factor = c("Attrition","BusinessTravel","Department","EducationField","EnvironmentSatisfaction",
"Gender","JobInvolvement","JobLevel","JobRole","JobSatisfaction","MaritalStatus","NumCompaniesWorked",
"OverTime","PerformanceRating","RelationshipSatisfaction","StockOptionLevel","TrainingTimesLastYear",
"WorkLifeBalance")
employeeData[cols.to.factor] = lapply(employeeData[cols.to.factor],factor)
#Load in libraries
library(lattice)
library(caret)
library(mlbench)
#prepare training scheme
control = trainControl(method="repeatedcv", number=10, repeats=3)
#train the model
model = train(Attrition~.,data=employeeData,method="lvq",preProcess="scale", trControl=control)
#estimate variable importance
importance = varImp(model,scale=FALSE)
#summarize importance
print(importance)
#plot importance
plot(importance)
#Load NB libraries
library(e1071)
#select variables decided to predict Attrition
data.nb = employeeData %>% select(Attrition, OverTime, MonthlyIncome, TotalWorkingYears, YearsAtCompany, StockOptionLevel, MaritalStatus, JobLevel, YearsInCurrentRole, YearsWithCurrManager, Age, JobInvolvement, JobSatisfaction, JobRole, Department,Education, WorkLifeBalance, EnvironmentSatisfaction)
set.seed(12)
splitPercent = 0.80
trainIndex = sample(1:dim(data.nb)[1],round(splitPercent * dim(data.nb)[1]))
train.nb = data.nb[trainIndex,]
test.nb = data.nb[-trainIndex,]
model.nb = naiveBayes(Attrition~.,data=train.nb, laplace = 1)
predict.nb = predict(model.nb,test.nb)
table(predict.nb,test.nb$Attrition)
confusionMatrix(predict.nb,test.nb$Attrition)
#Load library to run stepwise regression method to choose an optimal simple model
library(MASS)
#Build the model with internel verfication
set.seed(24)
train.control <- trainControl(method = "cv", number = 10)
step.model = train(MonthlyIncome~., data=employeeData,
method="lmStepAIC",
trControl = train.control,
trace=FALSE)
#Model Accuracy
step.model$results
step.model$finalModel
summary(step.model$finalModel)
?sd
?pt
pt(0.95,49)
qt(0.95,49)
?qt
qt(0.975,473.85)
qnorm(0.975,13)
?qnorm
qnorm(0.975,0,1)
?pf
pt(1.06,1,280)
pt(5,1,280)
pf(1.06,1,280)
1- pf(1.06,1,280)
?pt
pt(2.84,14)
1-pt(2.84,14)
1-2*pt(2.84,14)
(1-pt(2.84,14))*2
qnorm(2.84,0,1)
?qnorm
qnorm(2.84)
pnorm(2.84)
1- pnorm(2.84)
2^(1.249-2.594*0.031)
pf(1.06,1,280)
qt(0.95,1000000000)
qnorm(0.95)
qt(0.95,100)
qt(0.95,30)
qt(0.025,30)
qt(0.025,30)
qnorm(0.025,30)
qnorm(0.025)
qt(0.025,100)
qt(0.025,1000000)
qt(0.025,10000000000)
qt(0.45,1000000000000)
qnorm(0.45)
qnorm(0.5)
qt(0.5,1000000000000)
qnorm(0.6)
qt(0.6,1000)
qt(0.6,100)
exp(0.1975)
9.381-8.667
0.714/9
0.079/0.619
pf(0.128,9,14)
1-pf(0.128,9,14)
1.99979-0.001126
0.0998499+0.0102828
1.99979-0.0001084
0.0998499+0.0303267
exp(1.999789181)
Exp(-0.001083732)
exp(-0.001083732)
exp(0.03032665)
231-7
?qt
qt(0.995,224)
0.200127253+0.00013768*2.598
0.200127253-0.00013768*2.598
qt(0.975,224)
1.998664+0.1101327*9.4+0.2001273*2
exp(3.424166)
1.998664+0.1101327*9.4+0.2001273*3
exp(3.634293)
exp(3.627926)
exp(3.640658)
0.010282784-0.030326651
(4+9-8.2)*10^08
sqrt(4.8*10^(-8))
qt(0.975,224)
-0.02004387+1.97*0.000219
-0.02004387-1.97*0.000219
0.714/5
9.381-9.667
9.381-8.667
0.1428/0.619
1-pf(0.231,5,14)
library(tidyverse)
library(ggplot2)
library(GGally)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project1")
# Directly import cleaned dataset and start from here
CarData = read.csv("CleanedCarData.csv")
# Convert factor variables
variables.to.factor = c("Make","Model","EngineFuelType","TransmissionType","DrivenWheels","MarketCategory","VehicleSize","VehicleStyle","Crossover","Diesel","Exotic","Luxury","HighPerformance","FactoryTuner","Performance","FlexFuel","Hatchback","Hybrid","N_A")
CarData[variables.to.factor] = lapply(CarData[variables.to.factor],factor)
# Try Log transformation on MSRP
CarData$LogMSRP = log(CarData$MSRP)
#Try LogPopularity
CarData$LogPopularity = log(CarData$Popularity)
# Drop MSRP and Popularity from the original Dataset
CarData<-CarData[,-c(15,16)]
# Drop MarketCategory - 72 levels are not useful for prediction
CarData <-CarData[,-10]
str(CarData)
set.seed(111)
train.test.index<-sample(1:dim(CarData)[1],10723,replace=F)
train.test = CarData[train.test.index,]
validate = CarData[-train.test.index,]
set.seed(112)
train.index = sample(1:dim(train.test)[1],9531,replace=F)
train = train.test[train.index,]
test = train.test[-train.index,]
reg.fwd=regsubsets(LogMSRP~.,data=train,method="forward",nvmax=26)
# Use Stepwise variable selection method to select possible variables as predictors
library(leaps)
reg.fwd=regsubsets(LogMSRP~.,data=train,method="forward",nvmax=26)
summary(reg.fwd)$adjr2
summary(reg.fwd)$rss
summary(reg.fwd)$bic
par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:26,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)
plot(1:27,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)
par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:27,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)
adjr2<-summary(reg.fwd)$adjr2
plot(1:26,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)
adjr2<-summary(reg.fwd)$adjr2
plot(1:27,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss
plot(1:26,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss
plot(1:27,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)
which(bics==min(bics))
coef(reg.fwd,27)
reg.fwd=regsubsets(LogMSRP~.,data=train,method="forward",nvmax=150)
par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:151,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)
adjr2<-summary(reg.fwd)$adjr2
plot(1:151,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)
summary(train)
CarData$Make
summary(CarData$Make)
summary(CarData$Model)
summary(CarData$EngineFuelType)
reg.fwd=regsubsets(LogMSRP~.,data=train,method="forward",nvmax=500)
par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:501,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)
reg.fwd=regsubsets(LogMSRP~.,data=train,method="forward",nvmax=1000)
par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:1001,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)
bics
reg.fwd=regsubsets(LogMSRP~.,data=train,method="forward",nvmax=955)
par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:956,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)
adjr2<-summary(reg.fwd)$adjr2
plot(1:956,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss
plot(1:956,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)
which(bics==min(bics))
coef(reg.fwd,676)
# By using Forward method, 676 predictors seem to generate the smallest BIC number
# This model has selected predictors below:
# Make+Model+EngineFuelType+TransmissionType+VehicleSize+VehicleStyle+HighwayMPG+CityMPG+Luxury+FactoryTurner+Performance+FlexFuel+Hybrid+N_A+Exotic+Hatchback+LogPopularity
forward.regression = lm(LogMSRP~Make+Model+EngineFuelType+TransmissionType+VehicleSize+VehicleStyle+HighwayMPG+CityMPG+Luxury+FactoryTurner+Performance+FlexFuel+Hybrid+N_A+Exotic+Hatchback+LogPopularity,data=train)
# By using Forward method, 676 predictors seem to generate the smallest BIC number
# This model has selected predictors below:
# Make+Model+EngineFuelType+TransmissionType+VehicleSize+VehicleStyle+HighwayMPG+CityMPG+Luxury+FactoryTuner+Performance+FlexFuel+Hybrid+N_A+Exotic+Hatchback+LogPopularity
forward.regression = lm(LogMSRP~Make+Model+EngineFuelType+TransmissionType+VehicleSize+VehicleStyle+HighwayMPG+CityMPG+Luxury+FactoryTuner+Performance+FlexFuel+Hybrid+N_A+Exotic+Hatchback+LogPopularity,data=train)
summary(forward.regression)
par(mfrow=c(2,2))
plot(forward.regression)
library(olsrr)
ols_plot_cooksd_bar(forward.regression)
ols_plot_resid_stand(forward.regression)
# After observing the Cook's D, there is observation 4659, 9435 has extremely large Cook's D
# Observation 1523 also has large Cook's D
train[4659]
# After observing the Cook's D, there is observation 4659, 9435 has extremely large Cook's D
# Observation 1523 also has large Cook's D
train[4659,]
# After observing the Cook's D, there is observation 4659, 9435 has extremely large Cook's D
# Observation 1523 also has large Cook's D
train[4659,25]
exp(train[4659,25])
train%>% filter(Make=="Toyota"&Model=="Yaris iA")
train%>% filter(Make=="Toyota")
train%>% filter(Make=="Toyota") %>% select(Make,Model,Year,exp(LogMSRP))
train%>% filter(Make=="Toyota") %>% select(Make,Model,Year,LogMSRP)
# After observing the Cook's D, there is observation 4659, 9435 has extremely large Cook's D
# Observation 1523 also has large Cook's D
train[4659,] # Toyota Yaris iA
train[9435,]
exp(train[9435,25])
#Look at other Exotic Cars
train%>%filter(Exotic=="Yes")%>% select(Make,Model,Year,LogMSRP,LogPopularity)
library(tidyverse)
library(ggplot2)
library(GGally)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project1")
# Directly import cleaned dataset and start from here
CarData = read.csv("CleanedCarData.csv")
# Convert factor variables
variables.to.factor = c("Make","Model","EngineFuelType","TransmissionType","DrivenWheels","MarketCategory","VehicleSize","VehicleStyle","Crossover","Diesel","Exotic","Luxury","HighPerformance","FactoryTuner","Performance","FlexFuel","Hatchback","Hybrid","N_A")
CarData[variables.to.factor] = lapply(CarData[variables.to.factor],factor)
summary(CarData)
# Plot agsint numeric variables
CarData %>% select(MSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity) %>% ggpairs()
set.seed(111)
train.test.index<-sample(1:dim(CarData)[1],10723,replace=F)
train.test = CarData[train.test.index,]
validate = CarData[-train.test.index,]
set.seed(112)
train.index = sample(1:dim(train.test)[1],9531,replace=F)
train = train.test[train.index,]
test = train.test[-train.index,]
str(train)
CarData[,-10] #delete MarketCategory
CarData = CarData[,-10] #delete MarketCategory
#Without transformation, split dataset
set.seed(111)
train.test.index<-sample(1:dim(CarData)[1],10723,replace=F)
train.test = CarData[train.test.index,]
validate = CarData[-train.test.index,]
set.seed(112)
train.index = sample(1:dim(train.test)[1],9531,replace=F)
train = train.test[train.index,]
test = train.test[-train.index,]
str(train)
e
# Use Forward variable selection method to select possible variables as predictors
library(leaps)
reg.fwd=regsubsets(MSRP~.,data=train,method="forward",nvmax=1000)
bics<-summary(reg.fwd)$bic
bics
reg.fwd=regsubsets(MSRP~.,data=train,method="forward",nvmax=956)
bics<-summary(reg.fwd)$bic
bics
reg.fwd=regsubsets(MSRP~.,data=train,method="forward",nvmax=955)
par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:956,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)
adjr2<-summary(reg.fwd)$adjr2
plot(1:956,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss
plot(1:956,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)
# how many predictors gives the best BIC?
which(bics==min(bics))
coef(reg.fwd,345)
# By looking at this model, it has selected following predictors:
# Make+Model+EngineFuelType+EngineCylinders+DrivenWheels+VehicleStyle+HighwayMPG+CityMPG+FactoryTuner+Performance+Hybrid+N_A+Exotic+Hatchback
forward.regression = lm(MSRP~Make+Model+EngineFuelType+EngineCylinders+DrivenWheels+VehicleStyle+HighwayMPG+CityMPG+FactoryTuner+Performance+Hybrid+N_A+Exotic+Hatchback,data=train)
summary(forward.regression)
# This model has higher adjusted R-squared compared to the Log transformed model
par(mfrow=c(2,2))
plot(forward.regression)
library(olsrr)
ols_plot_cooksd_bar(forward.regression)
ols_plot_resid_stand(forward.regression)
# Cook's D plot looks okay - Much better than Log Transformed Plot
#Observation 577, 230 and 7329 seem to have problem on the residual plot
train[577,]
train[230,]
train[7329,]
# Try Backward elimination model.
reg.fwd=regsubsets(MSRP~.,data=train,method="backward",nvmax=1000)
# Try Backward elimination model.
reg.bwd=regsubsets(MSRP~.,data=train,method="backward",nvmax=1000)
summary(reg.bwd)$bic
########################################
# Try Backward elimination model.#######
########################################
reg.bwd=regsubsets(MSRP~.,data=train,method="backward",nvmax=955)
par(mfrow=c(1,3))
bics<-summary(reg.bwd)$bic
plot(1:956,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)
adjr2<-summary(reg.bwd)$adjr2
plot(1:956,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)
rss<-summary(reg.bwd)$rss
plot(1:956,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)
# how many predictors gives the best BIC?
which(bics==min(bics))
coef(reg.bwd,307)
# By looking at this model, it has selected following predictors:
# Make+Model+EngineFuelType+EngineCylinders+DrivenWheels+VehicleStyle+HighwayMPG+CityMPG+FactoryTuner+Performance+Hybrid+N_A+Exotic+Hatchback
forward.regression = lm(MSRP~Make+Model+EngineFuelType+EngineCylinders+DrivenWheels+VehicleStyle+HighwayMPG+CityMPG+FactoryTuner+Performance+Hybrid+N_A+Exotic+Hatchback,data=train)
# Predictor for backward elimination method based on lowest BIC value
# Make+Model+EngineFuelType+EngineCylinders+DrivenWheels+VehicleStyle+HighwayMPG+Performance+Exotic+Hatchback
backward.regression = lm(MSRP~Make+Model+EngineFuelType+EngineCylinders+DrivenWheels+VehicleStyle+HighwayMPG+Performance+Exotic+Hatchback,data=train)
summary(backward.regression)
# Forward select has higher adjusted R-squared compared to backward method
par(mfrow=c(2,2))
plot(backward.regression)
ols_plot_cooksd_bar(backward.regression)
ols_plot_resid_stand(backward.regression)
############################################
# Compare Forward vs. Backward on Test set #
############################################
# Test result
test.result = test$MSRP
str(test)
test = test[,-15]
#Forward
forwardPred = predict(forward.regression,test)
summary(CarData$Model)
dim(test)
dim(validate)
?sample_frac
library(tidyverse)
library(ggplot2)
library(GGally)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project1")
# Directly import cleaned dataset and start from here
CarData = read.csv("CleanedCarData.csv")
# Convert factor variables
variables.to.factor = c("Make","Model","EngineFuelType","TransmissionType","DrivenWheels","MarketCategory","VehicleSize","VehicleStyle","Crossover","Diesel","Exotic","Luxury","HighPerformance","FactoryTuner","Performance","FlexFuel","Hatchback","Hybrid","N_A")
CarData[variables.to.factor] = lapply(CarData[variables.to.factor],factor)
summary(CarData)
# Plot agsint numeric variables
CarData %>% select(MSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity) %>% ggpairs()
CarData = CarData[,-10] #delete MarketCategory

testData1$BsmtQual = ifelse(is.na(testData1$BsmtQual),"TA",testData1$BsmtQual)
testData1$BsmtQual = as.factor(testData1$BsmtQual)
testData1$BsmtFinType1 = as.character(testData1$BsmtFinType1)
testData1$BsmtFinType1 = ifelse(is.na(testData1$BsmtFinType1),"Unf",testData1$BsmtFinType1)
testData1$BsmtFinType1 = as.factor(testData1$BsmtFinType1)
testData1$KitchenQual = as.character(testData1$KitchenQual)
testData1$KitchenQual = ifelse(is.na(testData1$KitchenQual),"TA",testData1$KitchenQual)
testData1$KitchenQual = as.factor(testData1$KitchenQual)
testData1$MasVnrType = as.character(testData1$MasVnrType)
testData1$MasVnrType = ifelse(is.na(testData1$MasVnrType),"None",testData1$MasVnrType)
testData1$MasVnrType = as.factor(testData1$MasVnrType)
testData1$Exterior1st = as.character(testData1$Exterior1st)
testData1$Exterior1st = ifelse(testData1$Exterior1st=="Other","VinylSd",testData1$Exterior1st)
testData1$Exterior1st = as.factor(testData1$Exterior1st)
summary(testData1)
testData1$SalePrice = predict(custom.model,testData1)
testData1$SalePrice = exp(testData1$SalePrice)
cumstom.result = testData1 %>% dplyr::select(Id,SalePrice)
head(custom.result)
custom.result = testData1 %>% dplyr::select(Id,SalePrice)
head(custom.result)
write.csv(custom.result,"custom_model_Miller_YU.csv",row.names = FALSE)
variables.used
set.seed(116)
#create forward fit model
#write a program to generate 5 different combination to test forward selection model
iterations = 5
splitPerc = 0.9
total_RMSE = 0
for(i in 1:iterations){
print(i)
trainIndices = sample(1:dim(variables.used)[1],round(splitPerc * dim(variables.used)[1]))
train = variables.used[trainIndices,]
test = variables.used[-trainIndices,]
forward_fit = lm(lSalePrice~.-Id,data=train)
forward.model = ols_step_forward_aic(forward_fit,penter=0.15)
prediction = predict(forward.model$model,test)
squared_MSPE = mean((test$lSalePrice - prediction)^2)
temp_RMSE = sqrt(squared_MSPE)
total_RMSE = total_RMSE+temp_RMSE
}
#total RMSE = 0.1235978...
total_RMSE/iterations
library(tidyverse)
library(ggplot2)
library(olsrr)
library(GGally)
library(caret)
library(MASS)
library(leaps)
#Load in Dataset
#Call original data amesHouse
amesHouse = read.csv("/Users/mingyang/Desktop/SMU/StatisticalFoundation_Fall2020/MSDS6371Project/train.csv",header = TRUE)
#extract variables we're interested for Analysis 1
data1 = amesHouse %>% dplyr::select(SalePrice,GrLivArea,Neighborhood)%>%
filter(Neighborhood=="NAmes"|Neighborhood=="Edwards"|Neighborhood=="BrkSide")
#383 observations selected after filtering neighborhood of "NAmes", "Edwards" and "BrkSide"
#Convert Neighborhood to factors
data1$Neighborhood = as.factor(data1$Neighborhood)
#Plot and observe the relationship between SalePrice and GrLivArea
data1 %>% ggplot(aes(x=GrLivArea,y=SalePrice))+
geom_point()+ggtitle("Sale Price vs. Square Foot of Living area")+
xlab("Square Foot of Living Area")+
ylab("Sale Price")
#there are some outliers that may not have come from the same population of interest
#build model to identify the outliers
model1 = lm(SalePrice~GrLivArea,data=data1)
ols_plot_cooksd_bar(model1)
ols_plot_resid_stand(model1)
#Look at residual plots and cook's distance
plot(model1)
summary(model1)
#build new model to double check
model2 = lm(SalePrice~GrLivArea,data=data2)
ols_plot_cooksd_bar(model2)
#ols_plot_resid_lev(model1)
# Observation 339 has cook's distance larger than 5.6, Observation 131's cook's D is larger than 1 so it may due to the face it is a unique case.
# Observation 169 and 190 has standarlized residual greater than 4, these two observations can also potentially not coming from the same population of interest
# Since our sample is sufficiently large, deleting these four outliers won't make a huge difference.
# Call these new dataset without outliers data2
data2 = data1[-c(131,169,190,339),]
#build new model to double check
model2 = lm(SalePrice~GrLivArea,data=data2)
ols_plot_resid_stand(model2)
ols_plot_cooksd_bar(model3)
ols_plot_cooksd_bar(model2)
plot(model3)
plot(model2)
xlab("Square Foot of Living Area")+
ylab("Sale Price")
data2 %>% ggplot(aes(x=GrLivArea,y=SalePrice))+
geom_point()+ggtitle("Sale Price vs. Square Foot of Living area")+
xlab("Square Foot of Living Area")+
ylab("Sale Price")
#Cook's Distance within roughly 0.1, residual within 3, decent amount of observations beyond 2 (within 5%)
#Assumptions met, move on to include categorical variables
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
plot(model3)
data2 %>% ggplot(aes(x=GrLivArea,y=SalePrice,color=Neighborhood))+
geom_point()+ggtitle("Sale Price vs. Square Foot of Living area")+
xlab("Square Foot of Living Area")+
ylab("Sale Price")
plot(model3)
ols_plot_resid_stand(model3)
ols_plot_cooksd_bar(model3)
m
summary(model3)
summary(model2)
anova(model3)
confint(model3)
summary(model3)
data2$Neighborhood = relevel(data2$Neighborhood,ref="Edwards")
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
confint(model3)
# Change reference to BrkSide to get CI for slope
data2$Neighborhood = relevel(data2$Neighborhood,ref="BrkSide")
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
confint(model3)
# Change reference to NAmes to get CI for slope
data2$Neighborhood = relevel(data2$Neighborhood,ref="NAmes")
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
confint(model3)
# convert certain columns we need into factor in order to use multi regression model
cols.to.factor = c("MSSubClass","MSZoning","Street","Alley","LotShape","LandContour","Utilities",
"LotConfig","LandSlope","Neighborhood","Condition1","Condition2","BldgType",
"HouseStyle","OverallQual","OverallCond","RoofStyle","RoofMatl","Exterior1st",
"Exterior2nd","MasVnrType","ExterQual","ExterCond","Foundation","BsmtQual",
"BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2","Heating","HeatingQC",
"CentralAir","Electrical","KitchenQual","Functional","FireplaceQu","GarageType",
"GarageFinish","GarageQual","GarageCond","PavedDrive","PoolQC","Fence","MiscFeature",
"SaleType","SaleCondition","PoolArea","MoSold","YrSold","Fireplaces","FullBath")
amesHouse[cols.to.factor] = lapply(amesHouse[cols.to.factor],factor)
#Now exploring relationships between continuous variables
amesHouse %>% dplyr::select(SalePrice,LotFrontage,LotArea,YearBuilt,YearRemodAdd,MasVnrArea,BsmtFinSF1,BsmtFinSF2)%>%
ggpairs()
#Try Logrithmic on SalePrice
amesHouse$lSalePrice = log(amesHouse$SalePrice)
#explore relationship between log SalePrice
amesHouse %>% dplyr::select(lSalePrice,LotFrontage,LotArea,YearBuilt,YearRemodAdd,MasVnrArea,BsmtFinSF1,BsmtFinSF2)%>%
ggpairs()
#Move forward with next batch of exploration
amesHouse %>% dplyr::select(SalePrice,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea) %>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea) %>%
ggpairs()
#Move forward with next batch of exploration
amesHouse %>% dplyr::select(SalePrice,BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr,
TotRmsAbvGrd)%>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea) %>%
ggpairs()
#Move forward with next batch of exploration
amesHouse %>% dplyr::select(SalePrice,BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr,
TotRmsAbvGrd)%>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr,
TotRmsAbvGrd)%>%
ggpairs()
#The only variable seem to have good correlation with lSalePrice is FullBath,TotRmsAbvGrd.
#???If we should change BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr to factors???
#Keep going
amesHouse %>% dplyr::select(SalePrice,GarageYrBlt,GarageCars,GarageArea) %>% ggpairs()
amesHouse %>% dplyr::select(lSalePrice,GarageYrBlt,GarageCars,GarageArea) %>% ggpairs()
#The above variables all seem to have a good linear correlation with lSalePrice
#Keep going
amesHouse %>% dplyr::select(SalePrice,WoodDeckSF,OpenPorchSF,EnclosedPorch,X3SsnPorch,ScreenPorch,MiscVal)%>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,WoodDeckSF,OpenPorchSF,EnclosedPorch,X3SsnPorch,ScreenPorch,MiscVal)%>%
ggpairs()
#Loading libraries needed
library(tidyverse)
library(ggplot2)
#Loading in Employee data
employeeData = read.csv("/Users/mingyang/Desktop/SMU/DoingDS_Fall2020/CaseStudy2DDS/CaseStudy2-data.csv",header = TRUE)
summary(employeeData)
#See which column has only unique value
sapply(employeeData,function(col) length(unique(col)))
#Delete column that has only one unique value
to.be.deleted = which(sapply(employeeData,function(col) length(unique(col))==1))
employeeData = employeeData[,-to.be.deleted]
#Convert some values into factors
cols.to.factor = c("Attrition","BusinessTravel","Department","EducationField","EnvironmentSatisfaction",
"Gender","JobInvolvement","JobLevel","JobRole","JobSatisfaction","MaritalStatus","NumCompaniesWorked",
"OverTime","PerformanceRating","RelationshipSatisfaction","StockOptionLevel","TrainingTimesLastYear",
"WorkLifeBalance")
employeeData[cols.to.factor] = lapply(employeeData[cols.to.factor],factor)
#Load in libraries
library(lattice)
library(caret)
library(mlbench)
#prepare training scheme
control = trainControl(method="repeatedcv", number=10, repeats=3)
#train the model
model = train(Attrition~.,data=employeeData,method="lvq",preProcess="scale", trControl=control)
#estimate variable importance
importance = varImp(model,scale=FALSE)
#summarize importance
print(importance)
#plot importance
plot(importance)
#Load NB libraries
library(e1071)
#select variables decided to predict Attrition
data.nb = employeeData %>% select(Attrition, OverTime, MonthlyIncome, TotalWorkingYears, YearsAtCompany, StockOptionLevel, MaritalStatus, JobLevel, YearsInCurrentRole, YearsWithCurrManager, Age, JobInvolvement, JobSatisfaction, JobRole, Department,Education, WorkLifeBalance, EnvironmentSatisfaction)
set.seed(12)
splitPercent = 0.80
trainIndex = sample(1:dim(data.nb)[1],round(splitPercent * dim(data.nb)[1]))
train.nb = data.nb[trainIndex,]
test.nb = data.nb[-trainIndex,]
model.nb = naiveBayes(Attrition~.,data=train.nb, laplace = 1)
predict.nb = predict(model.nb,test.nb)
table(predict.nb,test.nb$Attrition)
confusionMatrix(predict.nb,test.nb$Attrition)
#Load library to run stepwise regression method to choose an optimal simple model
library(MASS)
#Build the model with internel verfication
set.seed(24)
train.control <- trainControl(method = "cv", number = 10)
step.model = train(MonthlyIncome~., data=employeeData,
method="lmStepAIC",
trControl = train.control,
trace=FALSE)
#Model Accuracy
step.model$results
step.model$finalModel
summary(step.model$finalModel)
?sd
?pt
pt(0.95,49)
qt(0.95,49)
?qt
qt(0.975,473.85)
qnorm(0.975,13)
?qnorm
qnorm(0.975,0,1)
?pf
pt(1.06,1,280)
pt(5,1,280)
pf(1.06,1,280)
1- pf(1.06,1,280)
?pt
pt(2.84,14)
1-pt(2.84,14)
1-2*pt(2.84,14)
(1-pt(2.84,14))*2
qnorm(2.84,0,1)
?qnorm
qnorm(2.84)
pnorm(2.84)
1- pnorm(2.84)
2^(1.249-2.594*0.031)
pf(1.06,1,280)
qt(0.95,1000000000)
qnorm(0.95)
qt(0.95,100)
qt(0.95,30)
qt(0.025,30)
qt(0.025,30)
qnorm(0.025,30)
qnorm(0.025)
qt(0.025,100)
qt(0.025,1000000)
qt(0.025,10000000000)
qt(0.45,1000000000000)
qnorm(0.45)
qnorm(0.5)
qt(0.5,1000000000000)
qnorm(0.6)
qt(0.6,1000)
qt(0.6,100)
exp(0.1975)
9.381-8.667
0.714/9
0.079/0.619
pf(0.128,9,14)
1-pf(0.128,9,14)
1.99979-0.001126
0.0998499+0.0102828
1.99979-0.0001084
0.0998499+0.0303267
exp(1.999789181)
Exp(-0.001083732)
exp(-0.001083732)
exp(0.03032665)
231-7
?qt
qt(0.995,224)
0.200127253+0.00013768*2.598
0.200127253-0.00013768*2.598
qt(0.975,224)
1.998664+0.1101327*9.4+0.2001273*2
exp(3.424166)
1.998664+0.1101327*9.4+0.2001273*3
exp(3.634293)
exp(3.627926)
exp(3.640658)
0.010282784-0.030326651
(4+9-8.2)*10^08
sqrt(4.8*10^(-8))
qt(0.975,224)
-0.02004387+1.97*0.000219
-0.02004387-1.97*0.000219
0.714/5
9.381-9.667
9.381-8.667
0.1428/0.619
1-pf(0.231,5,14)
library(tidyverse)
library(ggplot2)
library(GGally)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project1")
# Directly import cleaned dataset and start from here
CarData = read.csv("CleanedCarData.csv")
###################################################################################
# Restrict prediction range to over cars under 1 million... To improve precision###
###################################################################################
CarData = CarData %>% filter(MSRP < 1000000)
# Convert factor variables
variables.to.factor = c("Make","Model","EngineFuelType","TransmissionType","DrivenWheels","MarketCategory","VehicleSize","VehicleStyle","Crossover","Diesel","Exotic","Luxury","HighPerformance","FactoryTuner","Performance","FlexFuel","Hatchback","Hybrid","N_A")
CarData[variables.to.factor] = lapply(CarData[variables.to.factor],factor)
summary(CarData)
# Plot agsint numeric variables
# CarData %>% select(MSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity) %>% ggpairs()
CarData = CarData[,-10] #delete MarketCategory
# Try different way of sampling by Models
######### Try Something else to split data ############
# First 836 are Models that only have 5 observations or less - deleted 6 card over 1 million dollars
ModelList = CarData%>% group_by(Model) %>% mutate(ModelCount = n()) %>% arrange(ModelCount)
#arrange(ModelList,desc(count))%>%print(n = Inf)
# Can only split train/test/validate set on Models that has 6 observations or more
ModelList = ModelList[,-27]
Not.Able.To.Split = ModelList[(1:836),]
Able.To.Split = ModelList[-(1:836),]
# Split Train, test, validate based on able to Train Data...
set.seed(111)
validate.set.index<-sample(1:dim(Able.To.Split)[1],1191,replace=F)
validate = Able.To.Split[validate.set.index,]
remains1 = Able.To.Split[-validate.set.index,]
set.seed(112)
test.index = sample(1:dim(remains1)[1],1191,replace=F)
test = remains1[test.index,]
train = remains1[-test.index,]
# Joining train set with dataset that been left out
train = bind_rows(train,Not.Able.To.Split)
dim(train)
train = as.data.frame(train)
test = as.data.frame(test)
validate = as.data.frame(validate)
str(train)
str(CarData)
# Try Log transformation on MSRP see if it will improve correlations
train$logMSRP = log(train$MSRP)
train %>% select(logMSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity)%>%ggpairs()
# Explore numerical variables
train %>% select(MSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity)%>%ggpairs()
train %>% select(logMSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity)%>%ggpairs()
# Seems like LASSO has selected all predictors, only few levels within EngineFuel, TransmissionType, VehicleStyle got turned off
# Build model with predictors LASSO selected
log.model1 = lm(log(MSRP)~.,data=train)
# Check VIF
vif(log.model1)[,3]^2
library(car)
# Check VIF
vif(log.model1)[,3]^2
# Check VIF
summary(log.model1)
# It looks like logMSRP has increased correlation with Year but decreased correlation with EngineHP, EngineCylinders
# It's best to run LASSO and choose predictors on both with transformation and without Log transformation in order to identify which
# Model performs better
train = train[,-27]
# Since we want a easy to interperate model, we don't want to use predictor Make and Model that has too many levels in the model
train = train[,-(1:2)]
# Seems like LASSO has selected all predictors, only few levels within EngineFuel, TransmissionType, VehicleStyle got turned off
# Build model with predictors LASSO selected
log.model1 = lm(log(MSRP)~.,data=train)
# Check VIF
vif(log.model1)[,3]^2
# Check VIF
vif(log.model1)
summary(log.model1)
train %>% filter(Hatchback=="Yes")
# Check VIF
summary(log.model1)
# Hatchback provides NA on summary statistic, try remove that as predictor
log.model2 = lm(log(MSRP)~.-Hatchback,data=train)
summary(log.model2)
train %>% filter(TransmissionType=="DIRECT_DRIVE")
# TransmissionType and Hatchback provides NA on summary statistic, try remove them as predictors
log.model2 = lm(log(MSRP)~.-c(TransmissionnType,Hatchback),data=train)
# TransmissionType and Hatchback provides NA on summary statistic, try remove them as predictors
log.model2 = lm(log(MSRP)~.-c('TransmissionnType','Hatchback'),data=train)
# TransmissionType and Hatchback provides NA on summary statistic, try remove them as predictors
log.model2 = lm(log(MSRP)~.-('TransmissionnType','Hatchback'),data=train)
# TransmissionType and Hatchback provides NA on summary statistic, try remove them as predictors
log.model2 = lm(log(MSRP)~.- TransmissionType - Hatchback,data=train)
summary(log.model2)
plot(log.model2)
# Look at VIF
vif(log.model2)[,3]^2
# CityMPG has really high VIF remove it from model
log.model2 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG,data=train)
# CityMPG has really high VIF remove it from model
log.model3 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG,data=train)
# Look at VIF
vif(log.model3)[,3]^2
# NumberOfDoors has really high VIF remove from model
log.model4 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG - NumberOfDoors,data=train)
# Look at VIF
vif(log.model4)[,3]^2
summary(log.model4)
# Look at VIF
vif(log.model4)[,3]^2
# HighwayMPG still have over 10 VIF, remove from model
log.model5 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG - NumberOfDoors - HighwayMPG,data=train)
vif(log.model5)[,3]^2
summary(log.model5)
# Diesel is not significant, HighPerformance is not significant, Performance is not significant Remove from simple model
log.model6 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG - NumberOfDoors - HighwayMPG - Diesel - HighPerformance - Performance,data=train)
vif(log.model6)[,3]^2
summary(log.model5)
summary(log.model6)
# N_A is still not significant, remove from model
log.model7 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG - NumberOfDoors - HighwayMPG - Diesel - HighPerformance - Performance - N_A,data=train)
vif(log.model7)[,3]^2
summary(log.model7)
par(mfrow=c(2,2))
plot(log.model7)
library(olsrr)
ols_plot_cooksd_bar(log.model1)
ols_plot_resid_stand(log.model1)
#Dummy code categorical predictor variables
x1 <- model.matrix(MSRP~.,train)[,-1]
y1 <- train$MSRP
# Again, all predictors are selected, only few levels are turned off
regular.model1 = lm(MSRP~.,data=train)
summary(regular.model1)
# Delete Hatchback & TransmissionType... NA on summary table
regular.model2 = lm(MSRP~.-Hatchback-TransmissionType,data=train)
vif(regular.model2)[,3]^2
# remove CityMPG - highest VIF, larger than 29
regular.model3 = lm(MSRP~.-Hatchback-TransmissionType-CityMPG,data=train)
vif(regular.model3)[,3]^2
# remove NumberOfDoors
regular.model4 = lm(MSRP~.-Hatchback-TransmissionType-CityMPG-NumberOfDoors,data=train)
vif(regular.model4)[,3]^2
# HighwayMPG still over 10
regular.model5 = lm(MSRP~.-Hatchback-TransmissionType-CityMPG-NumberOfDoors-HighwayMPG,data=train)
vif(regular.model5)[,3]^2
summary(regular.model5)
# FlexFuel has the highest P value in terms of significance - delete
regular.model6 = lm(MSRP~.-Hatchback-TransmissionType-CityMPG-NumberOfDoors-HighwayMPG-FlexFuel,data=train)
vif(regular.model6)[,3]^2
summary(regular.model6)
#Diesel is high in p-value delete
regular.model7 = lm(MSRP~.-Hatchback-TransmissionType-CityMPG-NumberOfDoors-HighwayMPG-FlexFuel-Diesel,data=train)
vif(regular.model7)[,3]^2
summary(regular.model7)
par(mfrow=c(2,2))
plot(regular.model1)
# By Using StepwiseAIC variable selection method, the following predictors are selected
# Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+HighwayMPG+Hybrid+TransmissionType+EngineCylinders+Luxury+VehicleSize+FlexFuel+DrivenWheels+NumberOfDoors+Crossover+FactoryTuner+CityMPG+Popularity+N_A+Diesel
log.model2.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+HighwayMPG+Hybrid+TransmissionType+EngineCylinders+Luxury+VehicleSize+FlexFuel+DrivenWheels+NumberOfDoors+Crossover+FactoryTuner+CityMPG+Popularity+N_A+Diesel,data=train)
vif(log.model2.step)[,3]^2
summary(log.model2.step)
# TransmissionType has NA need to be deleted
log.model3.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+HighwayMPG+Hybrid+EngineCylinders+Luxury+VehicleSize+FlexFuel+DrivenWheels+NumberOfDoors+Crossover+FactoryTuner+CityMPG+Popularity+N_A+Diesel,data=train)
summary(log.model3.step)
vif(log.model3.step)[,3]^2
# CityMPG has the highest VIF - delete
log.model4.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+HighwayMPG+Hybrid+EngineCylinders+Luxury+VehicleSize+FlexFuel+DrivenWheels+NumberOfDoors+Crossover+FactoryTuner+Popularity+N_A+Diesel,data=train)
vif(log.model4.step)[,3]^2
# HighwayMPG still over 10, delete
log.model5.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+Hybrid+EngineCylinders+Luxury+VehicleSize+FlexFuel+DrivenWheels+NumberOfDoors+Crossover+FactoryTuner+Popularity+N_A+Diesel,data=train)
vif(log.model5.step)[,3]^2
summary(log.model5.step)
#FlexFuel has the highest insignificant P-value delete
log.model6.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+Hybrid+EngineCylinders+Luxury+VehicleSize+DrivenWheels+NumberOfDoors+Crossover+FactoryTuner+Popularity+N_A+Diesel,data=train)
summary(log.model6.step)
#Diesel has the highest insignificant P-value delete
log.model7.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+Hybrid+EngineCylinders+Luxury+VehicleSize+DrivenWheels+NumberOfDoors+Crossover+FactoryTuner+Popularity+N_A,data=train)
summary(log.model7.step)
#N_A has the highest insignificant P-value delete
log.model8.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+Hybrid+EngineCylinders+Luxury+VehicleSize+DrivenWheels+NumberOfDoors+Crossover+FactoryTuner+Popularity,data=train)
summary(log.model8.step)
par(mfrow=c(2,2))
plot(log.model2.step)
plot(log.model8.step)
summary(log.model7)
# LASSO and STEPWISE came to slight different Model in terms of predictor and r-squared value.
# LASSO r-squared 0.8426, adjusted r-squared 0.8419
# Stepwise r-squared 0.843, adjusted r-squared 0.8424
#compare test ASE results between stepwise model and LASSO
test = test[,-(1:2)]
testMSRP = test$MSRP # GET real result
test = test[,-13] # Delete MSRP from test
# Calculate stepwise test ASE first
pred.test.step = predict(log.model8.step,test)
# exp values to return to original value
pred.test.step = exp(pred.test.step)
test_ASE_STEP = mean((testMSRP-pred.test.step)^2)
test_ASE_STEP #497620223
#Calculate LASSO test ASE
pred.test.lasso = predict(log.model7,test)
pred.test.lasso = exp(pred.test.lasso)
test_ASE_LASSO = mean((testMSRP-pred.test.lasso)^2)
test_ASE_LASSO #499722642
summary(log.model8.step)

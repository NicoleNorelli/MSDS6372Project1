confusionMatrix(predict.nb,test.nb$Attrition)
#Load library to run stepwise regression method to choose an optimal simple model
library(MASS)
#Build the model with internel verfication
set.seed(24)
train.control <- trainControl(method = "cv", number = 10)
step.model = train(MonthlyIncome~., data=employeeData,
method="lmStepAIC",
trControl = train.control,
trace=FALSE)
#Model Accuracy
step.model$results
step.model$finalModel
summary(step.model$finalModel)
?sd
?pt
pt(0.95,49)
qt(0.95,49)
?qt
qt(0.975,473.85)
qnorm(0.975,13)
?qnorm
qnorm(0.975,0,1)
?pf
pt(1.06,1,280)
pt(5,1,280)
pf(1.06,1,280)
1- pf(1.06,1,280)
?pt
pt(2.84,14)
1-pt(2.84,14)
1-2*pt(2.84,14)
(1-pt(2.84,14))*2
qnorm(2.84,0,1)
?qnorm
qnorm(2.84)
pnorm(2.84)
1- pnorm(2.84)
2^(1.249-2.594*0.031)
pf(1.06,1,280)
qt(0.95,1000000000)
qnorm(0.95)
qt(0.95,100)
qt(0.95,30)
qt(0.025,30)
qt(0.025,30)
qnorm(0.025,30)
qnorm(0.025)
qt(0.025,100)
qt(0.025,1000000)
qt(0.025,10000000000)
qt(0.45,1000000000000)
qnorm(0.45)
qnorm(0.5)
qt(0.5,1000000000000)
qnorm(0.6)
qt(0.6,1000)
qt(0.6,100)
exp(0.1975)
9.381-8.667
0.714/9
0.079/0.619
pf(0.128,9,14)
1-pf(0.128,9,14)
1.99979-0.001126
0.0998499+0.0102828
1.99979-0.0001084
0.0998499+0.0303267
exp(1.999789181)
Exp(-0.001083732)
exp(-0.001083732)
exp(0.03032665)
231-7
?qt
qt(0.995,224)
0.200127253+0.00013768*2.598
0.200127253-0.00013768*2.598
qt(0.975,224)
1.998664+0.1101327*9.4+0.2001273*2
exp(3.424166)
1.998664+0.1101327*9.4+0.2001273*3
exp(3.634293)
exp(3.627926)
exp(3.640658)
0.010282784-0.030326651
(4+9-8.2)*10^08
sqrt(4.8*10^(-8))
qt(0.975,224)
-0.02004387+1.97*0.000219
-0.02004387-1.97*0.000219
0.714/5
9.381-9.667
9.381-8.667
0.1428/0.619
1-pf(0.231,5,14)
# Explore numerical variables
train %>% select(MSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity)%>%ggpairs()
# Try Log transformation on MSRP see if it will improve correlations
train$logMSRP = log(train$MSRP)
train %>% select(logMSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity)%>%ggpairs()
# It looks like logMSRP has increased correlation with Year but decreased correlation with EngineHP, EngineCylinders
# It's best to run LASSO and choose predictors on both with transformation and without Log transformation in order to identify which
# Model performs better
train = train[,-27]
#str(train)
library(glmnet)
library(car)
# Since we want a easy to interperate model, we don't want to use predictor Make and Model that has too many levels in the model
train = train[,-(1:2)]
str(train)
#Dummy code categorical predictor variables
x <- model.matrix(MSRP~.,train)[,-1]
y <- log(train$MSRP)
set.seed(123) # Need a seed number to keep same results for LASSO
cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
par(mfrow=c(1,1))
plot(cv.out)
# Look at coefficients
coef(cv.out, cv.out$lambda.min)
# Seems like LASSO has selected all predictors, only few levels within EngineFuel, TransmissionType, VehicleStyle got turned off
# Build model with predictors LASSO selected
log.model1 = lm(log(MSRP)~.,data=train)
# Check VIF
summary(log.model1)
# TransmissionType and Hatchback provides NA on summary statistic, try remove them as predictors
log.model2 = lm(log(MSRP)~.- TransmissionType - Hatchback,data=train)
summary(log.model2)
# Look at VIF
vif(log.model2)[,3]^2
# CityMPG has really high VIF remove it from model
log.model3 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG,data=train)
# Look at VIF
vif(log.model3)[,3]^2
# NumberOfDoors has really high VIF remove from model
log.model4 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG - NumberOfDoors,data=train)
# Look at VIF
vif(log.model4)[,3]^2
# HighwayMPG still have over 10 VIF, remove from model
log.model5 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG - NumberOfDoors - HighwayMPG,data=train)
vif(log.model5)[,3]^2
summary(log.model5)
# HighPerformance has the least statistical significance - delete
log.model6 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG - NumberOfDoors - HighwayMPG - HighPerformance, data=train)
vif(log.model6)[,3]^2
summary(log.model6)
# Diesel has the least statistical significance - delete
log.model6 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG - NumberOfDoors - HighwayMPG - HighPerformance - Diesel, data=train)
vif(log.model6)[,3]^2
summary(log.model6)
# N_A has the least statistical significance - delete
log.model6 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG - NumberOfDoors - HighwayMPG - HighPerformance - Diesel - N_A, data=train)
vif(log.model6)[,3]^2
summary(log.model6)
# Performance is still not significant, remove from model
log.model7 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG - NumberOfDoors - HighwayMPG - HighPerformance - Diesel - N_A - Performance, data=train)
vif(log.model7)[,3]^2
summary(log.model7)
# Model 7 is a highly interpretable model with decent R-squared.
par(mfrow=c(2,2))
plot(log.model7)
library(olsrr)
ols_plot_cooksd_bar(log.model7)
ols_plot_resid_stand(log.model7)
# Residual looks okay~
# Now try MSRP without log transformation
#Dummy code categorical predictor variables
x1 <- model.matrix(MSRP~.,train)[,-1]
y1 <- train$MSRP
set.seed(123) # Need a seed number to keep same results for LASSO
cv.out1=cv.glmnet(x1,y1,alpha=1) #alpha=1 performs LASSO
par(mfrow=c(1,1))
plot(cv.out1)
# Look at coefficients
coef(cv.out1, cv.out1$lambda.min)
# Again, all predictors are selected, only few levels are turned off
regular.model1 = lm(MSRP~.,data=train)
summary(regular.model1)
# Delete Hatchback & TransmissionType... NA on summary table
regular.model2 = lm(MSRP~.-Hatchback-TransmissionType,data=train)
vif(regular.model2)[,3]^2
# remove CityMPG - highest VIF, larger than 29
regular.model3 = lm(MSRP~.-Hatchback-TransmissionType-CityMPG,data=train)
vif(regular.model3)[,3]^2
# remove NumberOfDoors
regular.model4 = lm(MSRP~.-Hatchback-TransmissionType-CityMPG-NumberOfDoors,data=train)
vif(regular.model4)[,3]^2
# HighwayMPG still over 10
regular.model5 = lm(MSRP~.-Hatchback-TransmissionType-CityMPG-NumberOfDoors-HighwayMPG,data=train)
vif(regular.model5)[,3]^2
summary(regular.model5)
# FlexFuel has the highest P value in terms of significance - delete
regular.model6 = lm(MSRP~.-Hatchback-TransmissionType-CityMPG-NumberOfDoors-HighwayMPG-FlexFuel,data=train)
vif(regular.model6)[,3]^2
summary(regular.model6)
#Diesel is high in p-value delete
regular.model7 = lm(MSRP~.-Hatchback-TransmissionType-CityMPG-NumberOfDoors-HighwayMPG-FlexFuel-Diesel,data=train)
vif(regular.model7)[,3]^2
summary(regular.model7)
par(mfrow=c(2,2))
plot(regular.model1) # Residual plot doesn't look as well as log(MSRP) residual so tranformation is necessary.
ols_plot_cooksd_bar(regular.model1)
ols_plot_resid_stand(regular.model1)
#############################################################################
# By comparison Constant variance looks a lot better with the log(MSRP)...###
# Since the assumption works best with this, next check with stepwise method to see what predictors will be selected
stepwise.model1 =lm(log(MSRP)~.,data=train)
ols_step_both_aic(stepwise.model1)
# By Using StepwiseAIC variable selection method, the following predictors are selected
# Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+HighwayMPG+Hybrid+TransmissionType+EngineCylinders+Luxury+VehicleSize+DrivenWheels+FlexFuel+NumberOfDoors+Crossover+FactoryTuner+Popularity+CityMPG+N_A
log.model2.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+HighwayMPG+Hybrid+TransmissionType+EngineCylinders+Luxury+VehicleSize+DrivenWheels+FlexFuel+NumberOfDoors+Crossover+FactoryTuner+Popularity+CityMPG+N_A,data=train)
summary(log.model2.step)
# TransmissionType has NA need to be deleted
log.model3.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+HighwayMPG+Hybrid+EngineCylinders+Luxury+VehicleSize+DrivenWheels+FlexFuel+NumberOfDoors+Crossover+FactoryTuner+Popularity+CityMPG+N_A,data=train)
summary(log.model3.step)
vif(log.model3.step)[,3]^2
# CityMPG has the highest VIF - delete
log.model4.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+HighwayMPG+Hybrid+EngineCylinders+Luxury+VehicleSize+DrivenWheels+FlexFuel+NumberOfDoors+Crossover+FactoryTuner+Popularity+N_A,data=train)
vif(log.model4.step)[,3]^2
# NumberOfDoors highest VIF - delete
log.model5.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+HighwayMPG+Hybrid+EngineCylinders+Luxury+VehicleSize+DrivenWheels+FlexFuel+Crossover+FactoryTuner+Popularity+N_A,data=train)
vif(log.model5.step)[,3]^2
# HighwayMPG still over 10 - delete
log.model5.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+Hybrid+EngineCylinders+Luxury+VehicleSize+DrivenWheels+FlexFuel+Crossover+FactoryTuner+Popularity+N_A,data=train)
vif(log.model5.step)[,3]^2
summary(log.model5.step)
#N_A has the highest insignificant P-value delete
log.model8.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+Hybrid+EngineCylinders+Luxury+VehicleSize+DrivenWheels+FlexFuel+Crossover+FactoryTuner+Popularity,data=train)
summary(log.model8.step)
par(mfrow=c(2,2))
plot(log.model8.step)
ols_plot_cooksd_bar(log.model8.step)
ols_plot_resid_stand(log.model8.step)
# LASSO and STEPWISE came to slight different Model in terms of predictor and r-squared value.
# LASSO r-squared 0.8426, adjusted r-squared 0.8419
# Stepwise r-squared 0.843, adjusted r-squared 0.8424
#compare test ASE results between stepwise model and LASSO
test = test[,-(1:2)]
testMSRP = test$MSRP # GET real result
test = test[,-13] # Delete MSRP from test
# Calculate stepwise test ASE first
pred.test.step = predict(log.model8.step,test)
# exp values to return to original value
pred.test.step = exp(pred.test.step)
test_ASE_STEP = mean((testMSRP-pred.test.step)^2)
sqrt(test_ASE_STEP) #test RMSE = 23903.2
#Calculate LASSO test ASE
pred.test.lasso = predict(log.model7,test)
pred.test.lasso = exp(pred.test.lasso)
test_ASE_LASSO = mean((testMSRP-pred.test.lasso)^2)
sqrt(test_ASE_LASSO) ##test RMSE = 23903.2
# By comparison, Stepwise model workflow provided the best model in terms of r-squared, adjusted r-squared and test ASE.
# Move forward with the analysis with log.model8.step
library(tidyverse)
library(ggplot2)
library(GGally)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project1")
# Directly import cleaned dataset and start from here
CarData = read.csv("CleanedCarData.csv")
###################################################################################
# Restrict prediction range to over cars under 1 million... To improve precision###
###################################################################################
CarData = CarData %>% filter(MSRP < 1000000)
# Convert factor variables
variables.to.factor = c("Make","Model","EngineFuelType","TransmissionType","DrivenWheels","MarketCategory","VehicleSize","VehicleStyle","Crossover","Diesel","Exotic","Luxury","HighPerformance","FactoryTuner","Performance","FlexFuel","Hatchback","Hybrid","N_A")
CarData[variables.to.factor] = lapply(CarData[variables.to.factor],factor)
summary(CarData)
# Plot agsint numeric variables
# CarData %>% select(MSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity) %>% ggpairs()
CarData = CarData[,-10] #delete MarketCategory
# Try different way of sampling by Models
######### Try Something else to split data ############
# First 836 are Models that only have 5 observations or less - deleted 6 card over 1 million dollars
ModelList = CarData%>% group_by(Model) %>% mutate(ModelCount = n()) %>% arrange(ModelCount)
#arrange(ModelList,desc(count))%>%print(n = Inf)
# Can only split train/test/validate set on Models that has 6 observations or more
ModelList = ModelList[,-27]
Not.Able.To.Split = ModelList[(1:836),]
Able.To.Split = ModelList[-(1:836),]
# Split Train, test, validate based on able to Train Data...
set.seed(111)
validate.set.index<-sample(1:dim(Able.To.Split)[1],1191,replace=F)
validate = Able.To.Split[validate.set.index,]
remains1 = Able.To.Split[-validate.set.index,]
set.seed(112)
test.index = sample(1:dim(remains1)[1],1191,replace=F)
test = remains1[test.index,]
train = remains1[-test.index,]
# Joining train set with dataset that been left out
train = bind_rows(train,Not.Able.To.Split)
dim(train)
train = as.data.frame(train)
test = as.data.frame(test)
validate = as.data.frame(validate)
str(train)
str(CarData)
tryplot = train%>% select(MSRP,EngineCylinders,EngineFuelType)
tryplot$logMSRP = log(tryplot$MSRP)
tryplot %>% ggplot(aes(x=EngineCylinders,y=logMSRP, color=EngineFuelType))+
geom_point(position="jitter")
tryplot %>% ggplot(aes(x=EngineCylinders,y=logMSRP, color=EngineFuelType))+
geom_point(position="jitter")+ scale_color_viridis_d()
x <- model.matrix(log(MSRP)~.+I(EngineHP^2)+EngineFuelType*EngineCylinders,train)[,-1]
y <- log(train$MSRP)
set.seed(123) # Need a seed number to keep same results for LASSO
cv.out3=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
par(mfrow=c(1,1))
plot(cv.out3)
coef(cv.out3, cv.out3$lambda.min)
# - Hatchback - Popularity
# Refit the model with predictors
lasso.lm.log.sq.int = lm(log(MSRP)~.+I(EngineHP^2)-Hatchback-Popularity+EngineFuelType*EngineCylinders,data=train)
summary(lasso.lm.log.sq.int) # R-Squared and Adjusted R-squared is slightly lower compared to regular MSRP using LASSO
# Check Assumptions
par(mfrow=c(2,2))
plot(lasso.lm.log.sq.int)
#####################################################
# Compare Forward vs. Backward on Test set to LASSO #
#####################################################
# Test result
test.result = test$MSRP
# str(test)
test = test[,-15] #delete MSRP for prediction
# LogMSRP with Second Degree EngineHP and interaction EngineCylinders*EngineFuelType
lasso.log.sq.int.pred = predict(lasso.lm.log.sq.int,test)
# Recover the value by exp
lasso.log.sq.int.pred = exp(lasso.log.sq.int.pred)
testASE_LASSO_LOG_Inter = mean((test.result-lasso.log.sq.int.pred)^2)
sqrt(testASE_LASSO_LOG_Inter) #test RMSE = 4957.532
# Year-Degree2 didn't get turned on neither did I(Year^3), I(EngineHP^2) did get turned on
# Following also get turned off...
# - Hatchback - Popularity
# Refit the model with predictors
lasso.lm.log = lm(log(MSRP)~.+I(EngineHP^2)-Hatchback-Popularity,data=train)
summary(lasso.lm.log) # R-Squared and Adjusted R-squared is slightly lower compared to regular MSRP using LASSO
summary(lasso.lm.log.sq.int) # R-Squared and Adjusted R-squared is slightly lower compared to regular MSRP using LASSO
summary(lasso.lm.log.sq.int) # R-Squared and Adjusted R-squared is slightly lower compared to regular MSRP using LASSO
summary(lasso.lm.log) # R-Squared and Adjusted R-squared is slightly lower compared to regular MSRP using LASSO
str(validate)
validate.result = validate$MSRP
# seems k=2 gives the best result according to our test ASE
fit = knnreg(trainX,trainY,k=2)
library(caret)
#getting training response
trainY = train$MSRP
#getting training predictors
trainX = train[,-c(1,2,4,7,8,10,11,15,16,17,18,19,20,21,22,23,24,25,26)]
# Doing the same for test
testY = test$MSRP
testX = test[,-c(1,2,4,7,8,10,11,15,16,17,18,19,20,21,22,23,24,25,26)]
# seems k=2 gives the best result according to our test ASE
fit = knnreg(trainX,trainY,k=2)
#N_A has the highest insignificant P-value delete
log.model8.step = lm(log(MSRP)~Year+EngineHP+Exotic+EngineFuelType+VehicleStyle+Hybrid+EngineCylinders+Luxury+VehicleSize+DrivenWheels+FlexFuel+Crossover+FactoryTuner+Popularity,data=train)
# Validate best model from objective 1 result
objective1.predict = predict(log.model8.step,validate)
# See forward test ASE
validateMSE_Object1 <- mean((validate.result-objective1.predict)^2)
sqrt(validateMSE_Object1) #test RMSE = 6786.323
# Validate best model from objective 2 result
objective2.reg.predict = predict(lasso.lm.log,validate)
validateMSE_object2.reg.predict = mean((validate.result-objective2.reg.predict)^2)
sqrt(validateMSE_object2.reg.predict) #
str(validate)
# Validate best model from KNN
objective2.knn.predict = predict(fit,validate)
validate.result = validate$MSRP
validate = validate[,-15]
# Validate best model from objective 1 result
objective1.predict = predict(log.model8.step,validate)
# See forward test ASE
validateMSE_Object1 <- mean((validate.result-objective1.predict)^2)
sqrt(validateMSE_Object1) #test RMSE = 55452.96
# Validate best model from objective 2 result
objective2.reg.predict = predict(lasso.lm.log,validate)
validateMSE_object2.reg.predict = mean((validate.result-objective2.reg.predict)^2)
sqrt(validateMSE_object2.reg.predict) #55452.94
str(validate)
# Validate best model from KNN
validateKNN = validate[,-c(1,2,4,7,8,10,11,15,16,17,18,19,20,21,22,23,24,25)]
objective2.knn.predict = predict(fit,validateKNN)
validateASE_KNN_k2 = mean((validate.result-validateASE_KNN_k2)^2)
validateASE_KNN_k2 = mean((validate.result-objective2.knn.predict)^2)
sqrt(validateASE_KNN_k2) # test RMSE = 6861.357
library(tidyverse)
library(ggplot2)
library(GGally)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project1")
# Directly import cleaned dataset and start from here
CarData = read.csv("CleanedCarData.csv")
###################################################################################
# Restrict prediction range to over cars under 1 million... To improve precision###
###################################################################################
CarData = CarData %>% filter(MSRP < 1000000)
# Convert factor variables
variables.to.factor = c("Make","Model","EngineFuelType","TransmissionType","DrivenWheels","MarketCategory","VehicleSize","VehicleStyle","Crossover","Diesel","Exotic","Luxury","HighPerformance","FactoryTuner","Performance","FlexFuel","Hatchback","Hybrid","N_A")
CarData[variables.to.factor] = lapply(CarData[variables.to.factor],factor)
summary(CarData)
# Plot agsint numeric variables
# CarData %>% select(MSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity) %>% ggpairs()
CarData = CarData[,-10] #delete MarketCategory
# Try different way of sampling by Models
######### Try Something else to split data ############
# First 836 are Models that only have 5 observations or less - deleted 6 card over 1 million dollars
ModelList = CarData%>% group_by(Model) %>% mutate(ModelCount = n()) %>% arrange(ModelCount)
#arrange(ModelList,desc(count))%>%print(n = Inf)
# Can only split train/test/validate set on Models that has 6 observations or more
ModelList = ModelList[,-27]
Not.Able.To.Split = ModelList[(1:836),]
Able.To.Split = ModelList[-(1:836),]
# Split Train, test, validate based on able to Train Data...
set.seed(111)
validate.set.index<-sample(1:dim(Able.To.Split)[1],1191,replace=F)
validate = Able.To.Split[validate.set.index,]
remains1 = Able.To.Split[-validate.set.index,]
set.seed(112)
test.index = sample(1:dim(remains1)[1],1191,replace=F)
test = remains1[test.index,]
train = remains1[-test.index,]
# Joining train set with dataset that been left out
train = bind_rows(train,Not.Able.To.Split)
dim(train)
train = as.data.frame(train)
test = as.data.frame(test)
validate = as.data.frame(validate)
str(train)
str(CarData)
# Try Log transformation on MSRP see if it will improve correlations
train$logMSRP = log(train$MSRP)
# It looks like logMSRP has increased correlation with Year but decreased correlation with EngineHP, EngineCylinders
# It's best to run LASSO and choose predictors on both with transformation and without Log transformation in order to identify which
# Model performs better
train = train[,-27]
# Since we want a easy to interperate model, we don't want to use predictor Make and Model that has too many levels in the model
train = train[,-(1:2)]
# Performance is still not significant, remove from model
log.model7 = lm(log(MSRP)~.- TransmissionType - Hatchback - CityMPG - NumberOfDoors - HighwayMPG - HighPerformance - Diesel - N_A - Performance, data=train)
vif(log.model7)[,3]^2
summary(log.model7)
validate.result = validate$MSRP
validate = validate[,-15]
# Validate best model from objective 1 result
objective1.predict = predict(log.model7,validate)
# See forward test ASE
validateMSE_Object1 <- mean((validate.result-objective1.predict)^2)
sqrt(validateMSE_Object1) #test RMSE = 55452.96
head(validate.result,100)
head(objective1.predict,100)
objective1.predict = exp(objective1.predict)
# See forward test ASE
validateMSE_Object1 <- mean((validate.result-objective1.predict)^2)
sqrt(validateMSE_Object1) #test RMSE = 55452.96
library(tidyverse)
library(ggplot2)
library(GGally)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project1")
# Directly import cleaned dataset and start from here
CarData = read.csv("CleanedCarData.csv")
###################################################################################
# Restrict prediction range to over cars under 1 million... To improve precision###
###################################################################################
CarData = CarData %>% filter(MSRP < 1000000)
# Convert factor variables
variables.to.factor = c("Make","Model","EngineFuelType","TransmissionType","DrivenWheels","MarketCategory","VehicleSize","VehicleStyle","Crossover","Diesel","Exotic","Luxury","HighPerformance","FactoryTuner","Performance","FlexFuel","Hatchback","Hybrid","N_A")
CarData[variables.to.factor] = lapply(CarData[variables.to.factor],factor)
summary(CarData)
# Plot agsint numeric variables
# CarData %>% select(MSRP,Year,EngineHP,EngineCylinders,NumberOfDoors,HighwayMPG,CityMPG,Popularity) %>% ggpairs()
CarData = CarData[,-10] #delete MarketCategory
# Try different way of sampling by Models
######### Try Something else to split data ############
# First 836 are Models that only have 5 observations or less - deleted 6 card over 1 million dollars
ModelList = CarData%>% group_by(Model) %>% mutate(ModelCount = n()) %>% arrange(ModelCount)
#arrange(ModelList,desc(count))%>%print(n = Inf)
# Can only split train/test/validate set on Models that has 6 observations or more
ModelList = ModelList[,-27]
Not.Able.To.Split = ModelList[(1:836),]
Able.To.Split = ModelList[-(1:836),]
# Split Train, test, validate based on able to Train Data...
set.seed(111)
validate.set.index<-sample(1:dim(Able.To.Split)[1],1191,replace=F)
validate = Able.To.Split[validate.set.index,]
remains1 = Able.To.Split[-validate.set.index,]
set.seed(112)
test.index = sample(1:dim(remains1)[1],1191,replace=F)
test = remains1[test.index,]
train = remains1[-test.index,]
# Joining train set with dataset that been left out
train = bind_rows(train,Not.Able.To.Split)
dim(train)
train = as.data.frame(train)
test = as.data.frame(test)
validate = as.data.frame(validate)
str(train)
str(CarData)
# Year-Degree2 didn't get turned on neither did I(Year^3), I(EngineHP^2) did get turned on
# Following also get turned off...
# - Hatchback - Popularity
# Refit the model with predictors
lasso.lm.log = lm(log(MSRP)~.+I(EngineHP^2)-Hatchback-Popularity,data=train)
summary(lasso.lm.log) # R-Squared and Adjusted R-squared is slightly lower compared to regular MSRP using LASSO
validate.result = validate$MSRP
validate = validate[,-15]
# Validate best model from objective 2 result
objective2.reg.predict = predict(lasso.lm.log,validate)
validateMSE_object2.reg.predict = mean((validate.result-objective2.reg.predict)^2)
validateMSE_object2.reg.predict = exp(validateMSE_object2.reg.predict)
objective2.reg.predict = exp(objective2.reg.predict)
validateMSE_object2.reg.predict = mean((validate.result-objective2.reg.predict)^2)
sqrt(validateMSE_object2.reg.predict) #test RMSE = 55452.94
